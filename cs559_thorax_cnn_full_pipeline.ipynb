{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a799f0cb",
   "metadata": {},
   "source": [
    "\n",
    "# CS 559 – Thoraxial Medical Image Classification using CNNs\n",
    "\n",
    "This notebook implements the full pipeline described in the project proposal:\n",
    "\n",
    "- Load and filter the **CoronaHack – Chest X-Ray Dataset** metadata  \n",
    "- Exclude all **COVID-19** images and keep only **Normal** vs **Pneumonia**  \n",
    "- Preprocess images and apply **data augmentation**  \n",
    "- Train a **CNN** for binary classification  \n",
    "- Evaluate using **accuracy, F1 score, ROC/AUC, confusion matrix**  \n",
    "- Extract deep features and train **traditional ML baselines**  \n",
    "  - Logistic Regression  \n",
    "  - SVM (RBF)  \n",
    "  - Random Forest  \n",
    "  - AdaBoost  \n",
    "\n",
    "> **Important:** Before running, set `BASE_IMG_DIR` to the folder that contains your actual chest X‑ray images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b25acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d0a7a",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Paths and configuration\n",
    "\n",
    "Edit this section to point to the correct locations of:\n",
    "\n",
    "- The `Chest_xray_Corona_Metadata.csv` file  \n",
    "- The base directory containing the actual image files  \n",
    "\n",
    "On Kaggle, the images usually sit in subfolders like `train/` and `test/`.  \n",
    "The metadata column `Dataset_type` specifies which split each image belongs to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the metadata CSV (update if needed)\n",
    "META_CSV_PATH = \"Chest_xray_Corona_Metadata.csv\"  # or an absolute path if preferred\n",
    "\n",
    "# Base directory that contains the image files.\n",
    "# For example (Kaggle default):\n",
    "# BASE_IMG_DIR = \"/kaggle/input/coronahack-chest-xraydataset/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "BASE_IMG_DIR = \"/path/to/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset\"\n",
    "\n",
    "# Image subfolders used for each Dataset_type in the metadata\n",
    "DATASET_TYPE_TO_SUBDIR = {\n",
    "    \"TRAIN\": \"train\",       # images in BASE_IMG_DIR/train\n",
    "    \"TEST\": \"test\",         # images in BASE_IMG_DIR/test\n",
    "    \"VALIDATION\": \"train\"   # some releases don't have a separate validation folder\n",
    "}\n",
    "\n",
    "# Image size and training hyperparameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d57a5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load and clean metadata\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load the metadata CSV.  \n",
    "2. Create a full image filepath for each row.  \n",
    "3. Filter to keep only **Normal** and **Pneumonia** (spelled \"Pnemonia\" in this dataset).  \n",
    "4. **Exclude COVID‑19** images using `Label_2_Virus_category == \"COVID-19\"`.  \n",
    "5. Map labels to binary (0 = Normal, 1 = Pneumonia).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad31eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load metadata\n",
    "meta = pd.read_csv(META_CSV_PATH)\n",
    "print(\"Metadata shape:\", meta.shape)\n",
    "print(meta.head())\n",
    "\n",
    "# Helper to build full filepath\n",
    "def build_filepath(row):\n",
    "    dataset_type = str(row[\"Dataset_type\"]).upper()\n",
    "    subdir = DATASET_TYPE_TO_SUBDIR.get(dataset_type, \"train\")\n",
    "    return os.path.join(BASE_IMG_DIR, subdir, row[\"X_ray_image_name\"])\n",
    "\n",
    "meta[\"filepath\"] = meta.apply(build_filepath, axis=1)\n",
    "\n",
    "# Filter only rows where Label is Normal or Pnemonia\n",
    "mask_binary = meta[\"Label\"].isin([\"Normal\", \"Pnemonia\"])\n",
    "binary_meta = meta[mask_binary].copy()\n",
    "\n",
    "# Exclude rows where Label_2_Virus_category is explicitly COVID-19\n",
    "binary_meta = binary_meta[binary_meta[\"Label_2_Virus_category\"] != \"COVID-19\"]\n",
    "\n",
    "print(\"After filtering (Normal vs Pneumonia, no COVID-19):\", binary_meta.shape)\n",
    "\n",
    "# Map labels to 0/1\n",
    "label_map = {\"Normal\": 0, \"Pnemonia\": 1}\n",
    "binary_meta[\"target\"] = binary_meta[\"Label\"].map(label_map)\n",
    "\n",
    "binary_meta[[\"X_ray_image_name\", \"Label\", \"Label_2_Virus_category\", \"Dataset_type\", \"filepath\", \"target\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c31533",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Train / validation / test split\n",
    "\n",
    "We use the provided `Dataset_type` column:\n",
    "\n",
    "- All rows with `Dataset_type == \"TEST\"` ⇒ **test set**  \n",
    "- All remaining rows (TRAIN or VALIDATION) ⇒ **train+val**  \n",
    "- Then we split train+val into **train** and **validation** sets using `train_test_split`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21720b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use Dataset_type for initial split\n",
    "is_test = binary_meta[\"Dataset_type\"].str.upper() == \"TEST\"\n",
    "df_test = binary_meta[is_test].copy()\n",
    "df_trainval = binary_meta[~is_test].copy()\n",
    "\n",
    "print(\"Train+Val size:\", df_trainval.shape[0])\n",
    "print(\"Test size:\", df_test.shape[0])\n",
    "\n",
    "# Split train+val into train and validation\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.2,\n",
    "    stratify=df_trainval[\"target\"],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "for name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    print(f\"{name} distribution:\\n\", df[\"target\"].value_counts(normalize=True), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9cbf5",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Data generators and augmentation\n",
    "\n",
    "We use `ImageDataGenerator` to:\n",
    "\n",
    "- Rescale pixel values to `[0, 1]`  \n",
    "- Apply random rotations, shifts, zooms, and horizontal flips on the **training** set  \n",
    "- Use only rescaling for **validation** and **test** sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fcfb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def make_generator(df, datagen, shuffle=True):\n",
    "    return datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col=\"filepath\",\n",
    "        y_col=\"target\",\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"rgb\",\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "train_gen = make_generator(df_train, train_datagen, shuffle=True)\n",
    "val_gen = make_generator(df_val, val_test_datagen, shuffle=False)\n",
    "test_gen = make_generator(df_test, val_test_datagen, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4f542",
   "metadata": {},
   "source": [
    "\n",
    "## 5. CNN architecture\n",
    "\n",
    "We implement a straightforward CNN with:\n",
    "\n",
    "- 3 × `(Conv2D + MaxPooling2D)` blocks  \n",
    "- A fully-connected layer with dropout  \n",
    "- A final sigmoid unit for binary classification  \n",
    "\n",
    "Optimizer: **Adam**  \n",
    "Loss: **binary cross-entropy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_cnn(input_shape=(224, 224, 3)):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn(input_shape=IMG_SIZE + (3,))\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43bd4a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Train the CNN\n",
    "\n",
    "We monitor validation loss and use early stopping + best model checkpointing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=4,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"cnn_best_model.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f392cb",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curves(history):\n",
    "    hist = history.history\n",
    "    epochs_range = range(1, len(hist[\"loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, hist[\"loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs_range, hist[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, hist[\"accuracy\"], label=\"Train Acc\")\n",
    "    plt.plot(epochs_range, hist[\"val_accuracy\"], label=\"Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773a08b",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Evaluation: accuracy, F1, ROC/AUC, confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, generator, set_name=\"set\"):\n",
    "    # Predict probabilities\n",
    "    probs = model.predict(generator)\n",
    "    y_prob = probs.ravel()\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    y_true = generator.classes\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"=== {set_name.upper()} RESULTS ===\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_true, y_pred, target_names=[\"Normal\", \"Pneumonia\"]))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(f\"{set_name.capitalize()} Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [\"Normal\", \"Pneumonia\"], rotation=45)\n",
    "    plt.yticks(tick_marks, [\"Normal\", \"Pneumonia\"])\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], \"d\"),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{set_name.capitalize()} ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": roc_auc\n",
    "    }\n",
    "\n",
    "cnn_val_metrics = evaluate_model(cnn_model, val_gen, set_name=\"validation\")\n",
    "cnn_test_metrics = evaluate_model(cnn_model, test_gen, set_name=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63760184",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Classical ML baselines using deep features\n",
    "\n",
    "To compare the CNN against traditional ML models, we:\n",
    "\n",
    "1. Use **MobileNetV2** (pretrained on ImageNet) as a **fixed feature extractor**.  \n",
    "2. Extract a feature vector for each image.  \n",
    "3. Train classical classifiers on those features:\n",
    "\n",
    "   - Logistic Regression  \n",
    "   - SVM (RBF)  \n",
    "   - Random Forest  \n",
    "   - AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_cnn = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",   # requires internet to download the first time\n",
    "    pooling=\"avg\",\n",
    "    input_shape=IMG_SIZE + (3,)\n",
    ")\n",
    "base_cnn.trainable = False\n",
    "\n",
    "def extract_features(df):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = row[\"filepath\"]\n",
    "        img = tf.keras.utils.load_img(img_path, target_size=IMG_SIZE)\n",
    "        x = tf.keras.utils.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = mobilenet_preprocess(x)\n",
    "\n",
    "        feat = base_cnn.predict(x, verbose=0)\n",
    "        features.append(feat.squeeze())\n",
    "        labels.append(row[\"target\"])\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "X_train_feat, y_train_feat = extract_features(df_train)\n",
    "X_val_feat, y_val_feat = extract_features(df_val)\n",
    "X_test_feat, y_test_feat = extract_features(df_test)\n",
    "\n",
    "print(\"Feature shapes:\", X_train_feat.shape, X_val_feat.shape, X_test_feat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b740456e",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Train classical classifiers\n",
    "\n",
    "We train and evaluate:\n",
    "\n",
    "- Logistic Regression  \n",
    "- SVM with RBF kernel  \n",
    "- Random Forest  \n",
    "- AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6327d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_eval_clf(clf, X_tr, y_tr, X_te, y_te, name=\"model\"):\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_prob = clf.predict_proba(X_te)[:, 1] if hasattr(clf, \"predict_proba\") else clf.decision_function(X_te)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    f1 = f1_score(y_te, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_te, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 score:\", f1)\n",
    "    print(\"AUC:\", roc_auc)\n",
    "    print(\"Classification report:\\n\", classification_report(y_te, y_pred, target_names=[\"Normal\", \"Pneumonia\"]))\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"auc\": roc_auc}\n",
    "\n",
    "results_baselines = {}\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "results_baselines[\"Logistic Regression\"] = train_and_eval_clf(\n",
    "    log_reg, X_train_feat, y_train_feat, X_test_feat, y_test_feat, name=\"Logistic Regression\"\n",
    ")\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", probability=True)\n",
    "results_baselines[\"SVM (RBF)\"] = train_and_eval_clf(\n",
    "    svm_clf, X_train_feat, y_train_feat, X_test_feat, y_test_feat, name=\"SVM (RBF)\"\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED)\n",
    "results_baselines[\"Random Forest\"] = train_and_eval_clf(\n",
    "    rf_clf, X_train_feat, y_train_feat, X_test_feat, y_test_feat, name=\"Random Forest\"\n",
    ")\n",
    "\n",
    "ada_clf = AdaBoostClassifier(n_estimators=200, random_state=RANDOM_SEED)\n",
    "results_baselines[\"AdaBoost\"] = train_and_eval_clf(\n",
    "    ada_clf, X_train_feat, y_train_feat, X_test_feat, y_test_feat, name=\"AdaBoost\"\n",
    ")\n",
    "\n",
    "results_baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f00f9e",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Results summary\n",
    "\n",
    "This section aggregates all metrics (CNN + baselines) into a single table for inclusion in the **Results** section of the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = []\n",
    "\n",
    "all_results.append({\n",
    "    \"Model\": \"CNN (End-to-end)\",\n",
    "    \"Accuracy (test)\": cnn_test_metrics[\"accuracy\"],\n",
    "    \"F1 (test)\": cnn_test_metrics[\"f1\"],\n",
    "    \"AUC (test)\": cnn_test_metrics[\"auc\"]\n",
    "})\n",
    "\n",
    "for name, metrics in results_baselines.items():\n",
    "    all_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy (test)\": metrics[\"accuracy\"],\n",
    "        \"F1 (test)\": metrics[\"f1\"],\n",
    "        \"AUC (test)\": metrics[\"auc\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
